{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bba60788",
   "metadata": {},
   "source": [
    "# Gradient descent and Newton's method\n",
    "\n",
    "(This exercise originates from S. Boyd's course Convex Optimization)\n",
    "\n",
    "Consider the unconstrained problem\n",
    "  \n",
    "  $$\\begin{array}{ll}minimize\\,\\,\\, f(x) = - \\sum_{i=1}^m \\log(1-a_i^T x)- \\sum_{i=1}^n \\log(1 - x_i^2),\\end{array}$$\n",
    "  \n",
    "  with variable $x \\in \\mathbf{R}^n$, and $\\mathbf{dom} f = \\{x \\;|\\; a_i^Tx \\le 1, ~i=1, \\ldots, m, ~|x_i| \\le 1, ~i=1, \\ldots, n\\}$. This is the problem of computing the analytic center of the set of linear inequalities\n",
    "  \n",
    "  $$a_i^Tx \\leq 1, \\quad i=1, \\ldots, m, \\qquad |x_i|\\leq 1,\\quad i=1, \\ldots, n.$$\n",
    " \n",
    " Note that we can choose $x^{(0)}=0$ as our initial point. You can generate instances of this problem by choosing $a_i$ from some distribution on $\\mathbf{R}^n$.\n",
    " \n",
    "\n",
    "1. Use the gradient descent method to solve the problem. Use backtracking line search (BLS) to choose the step size and a stopping criterion of the form $\\| \\nabla f(x)\\|_2^2 \\leq \\delta$. You need to choose the parameters $\\alpha,\\beta$ of BLS as well as $\\delta$ in a reasonable way. Plot the objective function and step length versus iteration number. (Once you have determined $p^\\star$ to high accuracy, you can also plot $f-p^\\star$ versus iteration.) Experiment with the backtracking parameters $\\alpha$ and $\\beta$ to see their effect on the total number of iterations required. \n",
    "2. Repeat using Newton&#39;s method, again with BLS and this time with a stopping criterion based on the square $\\lambda^2$ of Newton decrement. Produce the plots as for GD. Can you see the sharp distinction between the areas of slow and fast convergence for the Newton&#39;s method?\n",
    "\n",
    "Some notes and tips:\n",
    "* You can find the description of BLS in the slides for the Newton's method lecture. The exact same formulation can be used for GD.\n",
    "* The domain of the objective is not $\\mathbf{R}^n$. So, your BLS needs make sure that the chosen step length results in a point inside the domain (on top of satisfying the standard BLS condition). \n",
    "* You are going to have to find formulas for $\\nabla f(x)$ and $\\nabla^2 f(x)$ by hand using chain rule. If you really want to, you can use pytorch (or other similar tools) for this task, but it is probably not worth it. The GD code and the Newton code has to be your own.\n",
    "* In the Newton's algorithm do not invert the Hessian, use ``numpy.linalg.solve`` (or other linear system solver) instead."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
